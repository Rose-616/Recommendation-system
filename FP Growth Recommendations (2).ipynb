{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQSF3a_v9n4U",
        "outputId": "53f8caaf-fc6c-407f-db46-ad85e3decb44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPsKSYdW972s",
        "outputId": "00e0c7c8-37c2-400b-a948-d75076de472b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "pip install numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6h4MI0b9_zB",
        "outputId": "7a4143f6-7446-4ec8-dcc7-87f5441c59d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (75.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install setuptools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkPN56X8-DnV",
        "outputId": "91edcc7f-d231-450f-b46c-dc1155143b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_2eqyM79AwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd360af-c6fb-4754-943b-d83be42fea9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[order_number: bigint, item_id: string, item_name: string, core_category: string, store_id: string, created_at: date]\n",
            "Processing City: KARACHI\n",
            "+------------+--------------------+--------------------+-------------------+--------------------+----------+\n",
            "|order_number|             item_id|           item_name|      core_category|            store_id|created_at|\n",
            "+------------+--------------------+--------------------+-------------------+--------------------+----------+\n",
            "| 24091140039|  495406729127576347|Head & Shoulders ...|      Personal Care|19TE0fJ8dDAIHobnu...|2024-10-12|\n",
            "| 24091140039|  510597221483309850|Shan Bombay Birya...|Masala & Condiments|19TE0fJ8dDAIHobnu...|2024-10-12|\n",
            "| 24091140039|  850744270230871589|Shan Nihari Sache...|Masala & Condiments|19TE0fJ8dDAIHobnu...|2024-10-12|\n",
            "| 24091140039|  858265587021175186|Ariel Washing Pow...|              Safai|19TE0fJ8dDAIHobnu...|2024-10-12|\n",
            "| 24091140039|  515035393486259426|Shan Korma Sachet...|Masala & Condiments|19TE0fJ8dDAIHobnu...|2024-10-12|\n",
            "| 24091140039|  140996621036706816|Aashna Cooking Oi...|       Oil aur Ghee|19TE0fJ8dDAIHobnu...|2024-10-12|\n",
            "| 24091140039|  936197595410502643|Shan Chicken Kara...|Masala & Condiments|19TE0fJ8dDAIHobnu...|2024-10-12|\n",
            "| 24091140039|  922244048151319075|Colgate Regular 20gm|      Personal Care|19TE0fJ8dDAIHobnu...|2024-10-12|\n",
            "| 24091140039|5kUUsg5AGKKz6agFB...|Brite Max Washing...|              Safai|19TE0fJ8dDAIHobnu...|2024-10-12|\n",
            "| 24091140039|  358807783987689654|Shan Achari Gosht...|Masala & Condiments|19TE0fJ8dDAIHobnu...|2024-10-12|\n",
            "| 24088570620|       5904807559324|Tarang Tea Whiten...|              Dairy|1omaVsHpxzK1vm3ZI...|2024-10-12|\n",
            "| 24088570620|       5904808771740|Habib Banaspati G...|       Oil aur Ghee|1omaVsHpxzK1vm3ZI...|2024-10-12|\n",
            "| 24088570620|       5523409502364|Dalda Banaspati G...|       Oil aur Ghee|1omaVsHpxzK1vm3ZI...|2024-10-12|\n",
            "| 24088465187|  306463297527162744|Aashna Cooking Oi...|       Oil aur Ghee|1TKNBkUB2DXPvW6o3...|2024-10-12|\n",
            "| 24088465187|       5904808771740|Habib Banaspati G...|       Oil aur Ghee|1TKNBkUB2DXPvW6o3...|2024-10-12|\n",
            "| 24088465187|  380687603870650147|Coca-Cola 1L Pack...|          Beverages|1TKNBkUB2DXPvW6o3...|2024-10-12|\n",
            "| 24088465187|  126943296911486101|Mezan Royal Cooki...|       Oil aur Ghee|1TKNBkUB2DXPvW6o3...|2024-10-12|\n",
            "| 24085559921|       5889758920860|Habib Cooking Oil...|       Oil aur Ghee|4eQT829yBtx38aGSj...|2024-10-12|\n",
            "| 24085559921|  668383357090664202|Sting Red Berry 5...|          Beverages|4eQT829yBtx38aGSj...|2024-10-12|\n",
            "| 24087669085|  560848400304378680|Pepsi 500ml Pack ...|          Beverages|6UYHqotQthRoly5xi...|2024-10-12|\n",
            "+------------+--------------------+--------------------+-------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import StringIndexer, IndexToString\n",
        "from pyspark.ml.fpm import FPGrowth\n",
        "from datetime import timedelta\n",
        "from itertools import chain, combinations\n",
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RecommendationSystem\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.conf.set(\"fs.s3a.acl.default\", \"BucketOwnerFullControl\")\n",
        "spark.conf.set(\"fs.s3.canned.acl\", \"BucketOwnerFullControl\")\n",
        "spark.conf.set(\"fs.s3.acl.default\", \"BucketOwnerFullControl\")\n",
        "spark.conf.set(\"fs.s3a.acl\", \"bucket-owner-full-control\")\n",
        "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC+05:00\")\n",
        "\n",
        "# Destination table paths\n",
        "dst_table = \"recommendations/\"\n",
        "dst_table_csv = \"recommendations_csv/recommendations_national_\"\n",
        "\n",
        "\n",
        "def days_finder(df_order_temp, n_days, max_similar_items, min_items_confidence, pattern_proportion):\n",
        "    df_order_temp = df_order_temp.filter(F.col('created_at') >= F.date_sub(F.current_date(), n_days))\n",
        "    order_days_count = df_order_temp.count()\n",
        "    distinct_items = df_order_temp.select(F.countDistinct('item_id')).collect()[0][0]\n",
        "    orders_required = distinct_items * max_similar_items * min_items_confidence / pattern_proportion\n",
        "    orders_single_day = order_days_count / n_days\n",
        "    return 0 if orders_single_day == 0 else orders_required / orders_single_day\n",
        "\n",
        "\n",
        "def trim_min_days(df_order, n_days):\n",
        "    return df_order.filter(F.col('created_at') >= F.date_sub(F.current_date(), n_days))\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "\n",
        "    def train_encoder(self, df, col='id'):\n",
        "        stringIndexer = StringIndexer(inputCol=col, outputCol=\"id\", stringOrderType=\"frequencyDesc\")\n",
        "        self.model = stringIndexer.fit(df)\n",
        "\n",
        "    def encode_features(self, df):\n",
        "        return self.model.transform(df)\n",
        "\n",
        "    def decode_features(self, df, inputCol, outputCol):\n",
        "        inverter = IndexToString(inputCol=inputCol, outputCol=outputCol, labels=self.model.labels)\n",
        "        return inverter.transform(df)\n",
        "\n",
        "\n",
        "def transform_data_fp(df, b_col, p_col, d_col):\n",
        "    return df.groupby(b_col) \\\n",
        "        .agg(F.collect_list(p_col).alias(p_col),\n",
        "             F.collect_list(d_col).alias(d_col),\n",
        "             F.collect_list('id').alias('id')) \\\n",
        "        .withColumn(\"id\", F.array_distinct(\"id\"))\n",
        "\n",
        "\n",
        "def min_date(date_time_obj, behaviour_days):\n",
        "    return date_time_obj - timedelta(days=behaviour_days)\n",
        "\n",
        "\n",
        "def merge(pred_ids_arr, scores_arr):\n",
        "    item_score_dic = {}\n",
        "    for pred_ids, scores in zip(pred_ids_arr, scores_arr):\n",
        "        for p, s in zip(pred_ids, scores):\n",
        "            item_score_dic[p] = item_score_dic.get(p, 0) + s\n",
        "    return [k for k, v in sorted(item_score_dic.items(), key=lambda item: item[1], reverse=True)]\n",
        "\n",
        "\n",
        "def all_subsets(ss):\n",
        "    return chain(*map(lambda x: combinations(ss, x), range(0, len(ss) + 1)))\n",
        "\n",
        "\n",
        "def make_combs(item_list):\n",
        "    return [list(x) for x in all_subsets(item_list) if x]\n",
        "\n",
        "\n",
        "def top_10_items(item_list):\n",
        "    return item_list[:10]\n",
        "\n",
        "\n",
        "def in_range(min_date, max_date, date):\n",
        "    return min_date <= date <= max_date\n",
        "\n",
        "\n",
        "def main():\n",
        "    item_mapping = spark.read.csv(\"item_mappings.csv\", header=True, inferSchema=True)\n",
        "    dataset = spark.read.csv(\"dataset.csv\", header=True, inferSchema=True)\n",
        "    dataset.createOrReplaceTempView(\"dataset\")\n",
        "    print(dataset)\n",
        "    cities = ['KARACHI']  # Add more cities as needed\n",
        "    repartition = 10\n",
        "\n",
        "    for city in cities:\n",
        "        print(f\"Processing City: {city}\")\n",
        "\n",
        "        dataset_query = f\"\"\"\n",
        "            SELECT\n",
        "                order_number,\n",
        "                item_id,\n",
        "                item_name,\n",
        "                core_category,\n",
        "                store_id,\n",
        "                created_at\n",
        "            FROM dataset\n",
        "        \"\"\"\n",
        "        df_order_sql = spark.sql(dataset_query)\n",
        "        df_order_sql.show()\n",
        "\n",
        "        # Item mapping processing\n",
        "        df_items = item_mapping\n",
        "\n",
        "        # Hyperparameters\n",
        "        n_days = 40\n",
        "        behaviour_days = 40\n",
        "        min_confidence = 0.1\n",
        "        max_similar_items = 10\n",
        "        min_items_confidence = 10\n",
        "        pattern_proportion = 1 / 4\n",
        "        sorting_metric = 'confidence'\n",
        "\n",
        "        b_col = 'order_number'\n",
        "        p_col = 'item_id'\n",
        "        d_col = 'item_name'\n",
        "\n",
        "        df_order_sql = df_order_sql.withColumn('created_at', F.to_timestamp('created_at'))\n",
        "        n_days = days_finder(df_order_sql, n_days, max_similar_items, min_items_confidence, pattern_proportion)\n",
        "        df_order = trim_min_days(df_order_sql, int(n_days))\n",
        "\n",
        "        enc = Encoder()\n",
        "        enc.train_encoder(df_items, 'item_id')\n",
        "        df_encoded = enc.encode_features(df_order)\n",
        "        df_fp = transform_data_fp(df_encoded, b_col, p_col, d_col)\n",
        "\n",
        "        fpGrowth = FPGrowth(itemsCol=\"id\", minSupport=0.001, minConfidence=0.1, numPartitions=64)\n",
        "        model = fpGrowth.fit(df_fp.repartition(repartition))\n",
        "\n",
        "        # Association rules\n",
        "        model_associations_df = model.associationRules \\\n",
        "            .withColumn('consequent', F.explode('consequent')) \\\n",
        "            .groupby('antecedent') \\\n",
        "            .agg(F.collect_list('consequent').alias('consequent'),\n",
        "                 F.collect_list(sorting_metric).alias(sorting_metric))\n",
        "\n",
        "        id_itemId_dic = {row['id']: row['item_id'] for row in enc.encode_features(df_items).collect()}\n",
        "        id_name_dic = {row['item_id']: row['item_name'] for row in df_items.collect()}\n",
        "\n",
        "        id_itemId_udf = F.udf(lambda ids: [id_itemId_dic[i] for i in ids], ArrayType(StringType()))\n",
        "        id_name_udf = F.udf(lambda ids: [id_name_dic[i] for i in ids], ArrayType(StringType()))\n",
        "        model_associations_df = model_associations_df \\\n",
        "            .withColumn('input_ids', id_itemId_udf('antecedent')) \\\n",
        "            .withColumn('pred_ids', id_itemId_udf('consequent')) \\\n",
        "            .withColumn('input_names', id_name_udf('input_ids')) \\\n",
        "            .withColumn('pred_names', id_name_udf('pred_ids')) \\\n",
        "            .select('input_names', 'pred_names', sorting_metric, 'input_ids', 'pred_ids')\n",
        "\n",
        "        model_associations_df.show()\n",
        "\n",
        "        # Output recommendations\n",
        "        recommendations_prd = model_associations_df.select('input_names', 'pred_names').dropDuplicates()\n",
        "        recommendations_prd.show()\n",
        "        recommendations_prd = recommendations_prd.withColumn(\"input_names\", concat_ws(\", \", \"input_names\"))\n",
        "        recommendations_prd = recommendations_prd.withColumn(\"pred_names\", concat_ws(\", \", \"pred_names\"))\n",
        "        recommendations_prd.write.csv(\"output\", header=True, mode=\"overwrite\")\n",
        "\n",
        "        # Uncomment to save recommendations\n",
        "        # recommendations_prd.repartition(repartition).write.csv(f\"{dst_table}{city}\", mode=\"overwrite\", header=True)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVglRIDR9dnk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}